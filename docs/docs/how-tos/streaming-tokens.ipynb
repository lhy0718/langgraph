{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["_\ud55c\uad6d\uc5b4\ub85c \uae30\uacc4\ubc88\uc5ed\ub428_\n"]}, {"cell_type": "markdown", "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53", "metadata": {}, "source": ["# \uadf8\ub798\ud504\uc5d0\uc11c LLM \ud1a0\ud070 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\ub294 \ubc29\ubc95\n", "\n", "!!! \uc815\ubcf4 \"\uc804\uc81c \uc870\uac74\"\n", "\n", "\uc774 \uac00\uc774\ub4dc\ub294 \ub2e4\uc74c \ub0b4\uc6a9\uc744 \uc798 \uc54c\uace0 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.\n", "\n", "- [\uc2a4\ud2b8\ub9ac\ubc0d](../../concepts/streaming/)\n", "- [\ucc44\ud305 \ubaa8\ub378](https://python.langchain.com/docs/concepts/chat_models/)\n", "\n", "LangGraph\ub85c LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uad6c\ucd95\ud560 \ub54c, LangGraph \ub178\ub4dc \ub0b4\uc5d0\uc11c LLM \ud638\ucd9c\ub85c\ubd80\ud130 \uac1c\ubcc4 LLM \ud1a0\ud070\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c `graph.stream(..., stream_mode=\"messages\")`\ub97c \ud1b5\ud574 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n", "\n", "```python\n", "from langgraph.graph import StateGraph\n", "from langchain_openai import ChatOpenAI\n", "\n", "model = ChatOpenAI()\n", "def call_model(state: State):\n", "    model.invoke(...)\n", "    ...\n", "\n", "graph = (\n", "    StateGraph(State)\n", "    .add_node(call_model)\n", "    ...\n", "    .compile()\n", "    \n", "for msg, metadata in graph.stream(inputs, stream_mode=\"messages\"):\n", "    print(msg)\n", "```\n", "\n", "\uc2a4\ud2b8\ub9ac\ubc0d\ub41c \ucd9c\ub825\uc740 `(\uba54\uc2dc\uc9c0 \uccad\ud06c, \uba54\ud0c0\ub370\uc774\ud130)` \ud615\uc2dd\uc758 \ud29c\ud50c\uc785\ub2c8\ub2e4:\n", "\n", "* \uba54\uc2dc\uc9c0 \uccad\ud06c\ub294 LLM\uc774 \uc2a4\ud2b8\ub9ac\ubc0d\ud55c \ud1a0\ud070\uc785\ub2c8\ub2e4.\n", "* \uba54\ud0c0\ub370\uc774\ud130\ub294 LLM\uc774 \ud638\ucd9c\ub41c \uadf8\ub798\ud504 \ub178\ub4dc\uc5d0 \ub300\ud55c \uc815\ubcf4\uc640 LLM \ud638\ucd9c \uba54\ud0c0\ub370\uc774\ud130\uac00 \ud3ec\ud568\ub41c \ub515\uc154\ub108\ub9ac\uc785\ub2c8\ub2e4.\n", "\n", "!!! \ub178\ud2b8 \"LangChain \uc5c6\uc774 \uc0ac\uc6a9\ud558\uae30\"\n", "\n", "LLM \ud1a0\ud070\uc744 **LangChain \uc5c6\uc774 \uc2a4\ud2b8\ub9ac\ubc0d\ud574\uc57c \ud558\ub294 \uacbd\uc6b0**, [`stream_mode=\"custom\"`](../streaming/#custom)\ub97c \uc0ac\uc6a9\ud558\uc5ec LLM \uacf5\uae09\uc790 \ud074\ub77c\uc774\uc5b8\ud2b8\ub85c\ubd80\ud130 \uc9c1\uc811 \ucd9c\ub825\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \ud559\uc2b5\ud558\ub824\uba74 [\uc544\ub798 \uc608\uc2dc](#example-without-langchain)\ub97c \ud655\uc778\ud558\uc2ed\uc2dc\uc624.\n", "\n", "!!! \uacbd\uace0 \"Python < 3.11\uc758 \ube44\ub3d9\uae30 \ucc98\ub9ac\"\n", "\n", "Python < 3.11\uc5d0\uc11c \ube44\ub3d9\uae30 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud560 \ub54c\ub294, \ud638\ucd9c\ud560 \ub54c `RunnableConfig`\ub97c \ucc44\ud305 \ubaa8\ub378\uc5d0 \uc218\ub3d9\uc73c\ub85c \uc804\ub2ec\ud574\uc57c \ud569\ub2c8\ub2e4: `model.ainvoke(..., config)`.\n", "\uc2a4\ud2b8\ub9bc \uba54\uc11c\ub4dc\ub294 \ucf5c\ubc31\uc73c\ub85c \uc804\ub2ec\ub41c \uc2a4\ud2b8\ub9ac\ubc0d \ud2b8\ub808\uc774\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc911\ucca9\ub41c \ucf54\ub4dc\uc5d0\uc11c \ubaa8\ub4e0 \uc774\ubca4\ud2b8\ub97c \uc218\uc9d1\ud569\ub2c8\ub2e4. 3.11 \uc774\uc0c1\uc5d0\uc11c\ub294 [contextvars](https://docs.python.org/3/library/contextvars.html)\ub97c \ud1b5\ud574 \uc790\ub3d9\uc73c\ub85c \ucc98\ub9ac\ub418\uc9c0\ub9cc, 3.11 \uc774\uc804\uc5d0\ub294 [asyncio\uc758 \uc791\uc5c5](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task)\uc774 \uc801\uc808\ud55c `contextvar` \uc9c0\uc6d0\uc774 \ubd80\uc871\ud558\uc5ec \ucf5c\ubc31\uc774 \uc218\ub3d9\uc73c\ub85c \uad6c\uc131(config)\uc744 \uc804\ub2ec\ud574\uc57c\ub9cc \uc804\ud30c\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc544\ub798\uc758 `call_model` \ud568\uc218\uc5d0\uc11c \uc218\ud589\ub429\ub2c8\ub2e4.\n"]}, {"cell_type": "markdown", "id": "7cbd446a-808f-4394-be92-d45ab818953c", "metadata": {}, "source": ["## \uc124\uc815\n", "\n", "\uba3c\uc800 \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uc124\uce58\ud574\uc57c \ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833", "metadata": {}, "outputs": [], "source": ["%%capture --no-stderr\n", "%pip install --quiet -U langgraph langchain_openai\n"]}, {"cell_type": "markdown", "id": "d67b5425", "metadata": {}, "source": ["\ub2e4\uc74c\uc73c\ub85c, OpenAI(\uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud560 LLM)\uc5d0 \ub300\ud55c API \ud0a4\ub97c \uc124\uc815\ud574\uc57c \ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": null, "id": "a372be6f", "metadata": {}, "outputs": [], "source": ["import getpass\n", "import os\n", "\n", "\n", "def _set_env(var: str):\n", "    if not os.environ.get(var):\n", "        os.environ[var] = getpass.getpass(f\"{var}: \")\n", "\n", "\n", "_set_env(\"OPENAI_API_KEY\")\n"]}, {"cell_type": "markdown", "id": "cc088bbd", "metadata": {}, "source": ["<div class=\"admonition tip\">\n", "    <p class=\"admonition-title\">LangGraph \uac1c\ubc1c\uc744 \uc704\ud55c <a href=\"https://smith.langchain.com\">LangSmith</a> \uc124\uc815\ud558\uae30</p>\n", "    <p style=\"padding-top: 5px;\">\n", "        LangSmith\uc5d0 \uac00\uc785\ud558\uc5ec LangGraph \ud504\ub85c\uc81d\ud2b8\uc758 \ubb38\uc81c\ub97c \uc2e0\uc18d\ud558\uac8c \ubc1c\uacac\ud558\uace0 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uc138\uc694. LangSmith\ub97c \uc0ac\uc6a9\ud558\uba74 LangGraph\ub85c \uad6c\ucd95\ub41c LLM \uc571\uc744 \ub514\ubc84\uadf8, \ud14c\uc2a4\ud2b8 \ubc0f \ubaa8\ub2c8\ud130\ub9c1\ud558\uae30 \uc704\ud574 \ucd94\uc801 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 \u2014 \uc2dc\uc791\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 <a href=\"https://docs.smith.langchain.com\">\uc5ec\uae30</a>\ub97c \uc77d\uc5b4\ubcf4\uc138\uc694.\n", "    </p>\n", "</div>\n"]}, {"cell_type": "markdown", "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4", "metadata": {}, "source": ["!!! \uc8fc\uc758 \uc0ac\ud56d \uc218\ub3d9 \ucf5c\ubc31 \uc804\ud30c\n", "\n", "    \uc544\ub798\uc758 `call_model(state: State, config: RunnableConfig):`\uc5d0\uc11c, a) \ub178\ub4dc \ud568\uc218\uc5d0\uc11c [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)\ub97c \ubc1b\uace0, b) `model.ainvoke(..., config)`\uc758 \ub450 \ubc88\uc9f8 \uc778\uc790\ub85c \uc804\ub2ec\ud569\ub2c8\ub2e4. \uc774\ub294 \ud30c\uc774\uc36c 3.11 \uc774\uc0c1\uc5d0\uc11c\ub294 \uc120\ud0dd \uc0ac\ud56d\uc785\ub2c8\ub2e4.\n"]}, {"cell_type": "markdown", "id": "ad2c85b6-28f8-4c7f-843a-c05cb7fd7187", "metadata": {}, "source": ["## \uc608\uc81c\n"]}, {"cell_type": "markdown", "id": "afcbdd41-dff8-4118-8901-a619f91f3feb", "metadata": {}, "source": ["\uc544\ub798\uc5d0\uc11c\ub294 \ub2e8\uc77c \ub178\ub4dc\uc5d0\uc11c \ub450 \uac1c\uc758 LLM \ud638\ucd9c\uc744 \ud3ec\ud568\ud55c \uc608\uc81c\ub97c \ubcf4\uc5ec\ub4dc\ub9bd\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 3, "id": "7cc5905f-df82-4b31-84ad-2054f463aee8", "metadata": {}, "outputs": [], "source": ["from typing import TypedDict\n", "from langgraph.graph import START, StateGraph, MessagesState\n", "from langchain_openai import ChatOpenAI\n", "\n", "\n", "# Note: we're adding the tags here to be able to filter the model outputs down the line\n", "joke_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"joke\"])\n", "poem_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"poem\"])\n", "\n", "\n", "class State(TypedDict):\n", "    topic: str\n", "    joke: str\n", "    poem: str\n", "\n", "\n", "# highlight-next-line\n", "async def call_model(state, config):\n", "    topic = state[\"topic\"]\n", "    print(\"Writing joke...\")\n", "    # Note: Passing the config through explicitly is required for python < 3.11\n", "    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n", "    joke_response = await joke_model.ainvoke(\n", "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n", "        # highlight-next-line\n", "        config,\n", "    )\n", "    print(\"\\n\\nWriting poem...\")\n", "    poem_response = await poem_model.ainvoke(\n", "        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n", "        # highlight-next-line\n", "        config,\n", "    )\n", "    return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n", "\n", "\n", "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()\n"]}, {"cell_type": "code", "execution_count": 4, "id": "96050fba", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Writing joke...\n", "Why| was| the| cat| sitting| on| the| computer|?\n", "\n", "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n", "\n", "Writing poem...\n", "In| sun|lit| patches|,| sleek| and| sly|,|  \n", "|Wh|isk|ers| twitch| as| shadows| fly|.|  \n", "|With| velvet| paws| and| eyes| so| bright|,|  \n", "|They| dance| through| dreams|,| both| day| and| night|.|  \n", "\n", "|A| playful| p|ounce|,| a| gentle| p|urr|,|  \n", "|In| every| leap|,| a| soft| allure|.|  \n", "|Cur|led| in| warmth|,| a| silent| grace|,|  \n", "|Each| furry| friend|,| a| warm| embrace|.|  \n", "\n", "|Myst|ery| wrapped| in| fur| and| charm|,|  \n", "|A| soothing| presence|,| a| gentle| balm|.|  \n", "|In| their| gaze|,| the| world| slows| down|,|  \n", "|For| in| their| realm|,| we're| all| ren|own|.|"]}], "source": ["async for msg, metadata in graph.astream(\n", "    {\"topic\": \"cats\"},\n", "    # highlight-next-line\n", "    stream_mode=\"messages\",\n", "):\n", "    if msg.content:\n", "        print(msg.content, end=\"|\", flush=True)\n"]}, {"cell_type": "code", "execution_count": 5, "id": "bcdf561d-a5cd-4197-9c65-9ab8af85941f", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'langgraph_step': 1,\n", " 'langgraph_node': 'call_model',\n", " 'langgraph_triggers': ['start:call_model'],\n", " 'langgraph_path': ('__pregel_pull', 'call_model'),\n", " 'langgraph_checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n", " 'checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n", " 'ls_provider': 'openai',\n", " 'ls_model_name': 'gpt-4o-mini',\n", " 'ls_model_type': 'chat',\n", " 'ls_temperature': 0.7,\n", " 'tags': ['poem']}"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["metadata\n"]}, {"cell_type": "markdown", "id": "7db91f8d-3e17-47f4-b45e-c72bbbcbb5ed", "metadata": {}, "source": ["### \ud2b9\uc815 LLM \ud638\ucd9c\ub85c \ud544\ud130\ub9c1\n"]}, {"cell_type": "markdown", "id": "a3a72acd-98cc-43f6-9dbb-0e97d03d211b", "metadata": {}, "source": ["\uc6b0\ub9ac\ub294 \ubaa8\ub4e0 LLM \ud638\ucd9c\uc5d0\uc11c \ud1a0\ud070\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc2a4\ud2b8\ub9ac\ubc0d\ub41c \ud1a0\ud070\uc744 \ud2b9\uc815 LLM \ud638\ucd9c\ub9cc \ud3ec\ud568\ud558\ub3c4\ub85d \ud544\ud130\ub9c1\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc2a4\ud2b8\ub9ac\ubc0d\ub41c \uba54\ud0c0\ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uace0, \uc774\uc804\uc5d0 LLM\uc5d0 \ucd94\uac00\ud55c \ud0dc\uadf8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ubca4\ud2b8\ub97c \ud544\ud130\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 6, "id": "c9e0df34-6020-445e-8ecd-ca4239e9b22b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Writing joke...\n", "Why| was| the| cat| sitting| on| the| computer|?\n", "\n", "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n", "\n", "Writing poem...\n"]}], "source": ["async for msg, metadata in graph.astream(\n", "    {\"topic\": \"cats\"},\n", "    stream_mode=\"messages\",\n", "):\n", "    # highlight-next-line\n", "    if msg.content and \"joke\" in metadata.get(\"tags\", []):\n", "        print(msg.content, end=\"|\", flush=True)\n"]}, {"cell_type": "markdown", "id": "be8fd3d7-a227-41ad-bd08-7ef994ab291b", "metadata": {}, "source": ["## LangChain \uc5c6\uc774 \uc608\uc2dc\n"]}, {"cell_type": "code", "execution_count": 7, "id": "699b3bab-9da7-4f2a-8006-93289350d89d", "metadata": {}, "outputs": [], "source": ["from openai import AsyncOpenAI\n", "\n", "openai_client = AsyncOpenAI()\n", "model_name = \"gpt-4o-mini\"\n", "\n", "\n", "async def stream_tokens(model_name: str, messages: list[dict]):\n", "    response = await openai_client.chat.completions.create(\n", "        messages=messages, model=model_name, stream=True\n", "    )\n", "\n", "    role = None\n", "    async for chunk in response:\n", "        delta = chunk.choices[0].delta\n", "\n", "        if delta.role is not None:\n", "            role = delta.role\n", "\n", "        if delta.content:\n", "            yield {\"role\": role, \"content\": delta.content}\n", "\n", "\n", "# highlight-next-line\n", "async def call_model(state, config, writer):\n", "    topic = state[\"topic\"]\n", "    joke = \"\"\n", "    poem = \"\"\n", "\n", "    print(\"Writing joke...\")\n", "    async for msg_chunk in stream_tokens(\n", "        model_name, [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n", "    ):\n", "        joke += msg_chunk[\"content\"]\n", "        metadata = {**config[\"metadata\"], \"tags\": [\"joke\"]}\n", "        chunk_to_stream = (msg_chunk, metadata)\n", "        # highlight-next-line\n", "        writer(chunk_to_stream)\n", "\n", "    print(\"\\n\\nWriting poem...\")\n", "    async for msg_chunk in stream_tokens(\n", "        model_name, [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n", "    ):\n", "        poem += msg_chunk[\"content\"]\n", "        metadata = {**config[\"metadata\"], \"tags\": [\"poem\"]}\n", "        chunk_to_stream = (msg_chunk, metadata)\n", "        # highlight-next-line\n", "        writer(chunk_to_stream)\n", "\n", "    return {\"joke\": joke, \"poem\": poem}\n", "\n", "\n", "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()\n"]}, {"cell_type": "markdown", "id": "8af13d73-a0ea-44c0-a92e-28676cd164dd", "metadata": {}, "source": ["!!! \ub178\ud2b8 \"stream_mode=\"custom\"\"\n", "\n", "    LangChain \uc5c6\uc774 LLM \ud1a0\ud070\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \ub54c, [`stream_mode=\"custom\"`](../streaming/#stream-modecustom)\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 LangGraph \uc2a4\ud2b8\ub9ac\ubc0d \ucd9c\ub825\uc5d0 \ud3ec\ud568\ud560 LLM \uacf5\uae09\uc790 API\uc758 \ub370\uc774\ud130\ub97c \uba85\uc2dc\uc801\uc73c\ub85c \uc81c\uc5b4\ud560 \uc218 \uc788\uc73c\uba70, \ucd94\uac00 \uba54\ud0c0\ub370\uc774\ud130\ub97c \ud3ec\ud568\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 8, "id": "e977406d-7be6-4c9f-9185-5e5551f848f3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Writing joke...\n", "Why| was| the| cat| sitting| on| the| computer|?\n", "\n", "|Because| it| wanted| to| keep| an| eye| on| the|\n", "\n", "Writing poem...\n", " mouse|!|In| sun|lit| patches|,| they| stretch| and| y|awn|,|  \n", "|With| whispered| paws| at| the| break| of| dawn|.|  \n", "|Wh|isk|ers| twitch| in| the| morning| light|,|  \n", "|Sil|ken| shadows|,| a| graceful| sight|.|  \n", "\n", "|The| gentle| p|urr|s|,| a| soothing| song|,|  \n", "|In| a| world| of| comfort|,| where| they| belong|.|  \n", "|M|yster|ious| hearts| wrapped| in| soft|est| fur|,|  \n", "|F|eline| whispers| in| every| p|urr|.|  \n", "\n", "|Ch|asing| dreams| on| a| moon|lit| chase|,|  \n", "|With| a| flick| of| a| tail|,| they| glide| with| grace|.|  \n", "|Oh|,| playful| spirits| of| whisk|ered| cheer|,|  \n", "|In| your| quiet| company|,| the| world| feels| near|.|  |"]}], "source": ["async for msg, metadata in graph.astream(\n", "    {\"topic\": \"cats\"},\n", "    # highlight-next-line\n", "    stream_mode=\"custom\",\n", "):\n", "    print(msg[\"content\"], end=\"|\", flush=True)\n"]}, {"cell_type": "code", "execution_count": 9, "id": "0bdc1635-f424-4a5f-95db-e993bb16adb2", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'langgraph_step': 1,\n", " 'langgraph_node': 'call_model',\n", " 'langgraph_triggers': ['start:call_model'],\n", " 'langgraph_path': ('__pregel_pull', 'call_model'),\n", " 'langgraph_checkpoint_ns': 'call_model:3fa3fbe1-39d8-5209-dd77-0da38d4cc1c9',\n", " 'tags': ['poem']}"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["metadata\n"]}, {"cell_type": "markdown", "id": "a3afbbee-fab8-4c7f-ad26-094f8c8f4dd9", "metadata": {}, "source": ["\ud2b9\uc815 LLM \ud638\ucd9c\uc744 \ud544\ud130\ub9c1\ud558\ub824\uba74 \uc2a4\ud2b8\ub9ac\ubc0d \uba54\ud0c0\ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 10, "id": "fdeee9d9-2625-403a-9253-418a0feeed77", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Writing joke...\n", "\n", "\n", "Writing poem...\n", "In| shadows| soft|,| they| weave| and| play|,|  \n", "|With| whispered| paws|,| they| greet| the| day|.|  \n", "|Eyes| like| lantern|s|,| bright| and| keen|,|  \n", "|Guard|ians| of| secrets|,| unseen|,| serene|.|  \n", "\n", "|They| twist| and| stretch| in| sun|lit| beams|,|  \n", "|Ch|asing| the| echoes| of| half|-|formed| dreams|.|  \n", "|With| p|urring| songs| that| soothe| the| night|,|  \n", "|F|eline| spirits|,| pure| delight|.|  \n", "\n", "|On| windows|ills|,| they| perch| and| stare|,|  \n", "|Ad|vent|urers| bold| with| a| graceful| flair|.|  \n", "|In| every| leap| and| playful| bound|,|  \n", "|The| magic| of| cats|\u2014|where| love| is| found|.|"]}], "source": ["async for msg, metadata in graph.astream(\n", "    {\"topic\": \"cats\"},\n", "    stream_mode=\"custom\",\n", "):\n", "    # highlight-next-line\n", "    if \"poem\" in metadata.get(\"tags\", []):\n", "        print(msg[\"content\"], end=\"|\", flush=True)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 5}