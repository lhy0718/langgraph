{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["_\ud55c\uad6d\uc5b4\ub85c \uae30\uacc4\ubc88\uc5ed\ub428_\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "695d935e-b4fe-45a6-a061-a66d32cb832b", "metadata": {}, "source": ["# \ub3c4\uad6c \ub0b4\uc5d0\uc11c \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\ub294 \ubc29\ubc95\n", "\n", "!!! \uc815\ubcf4 \"\uc804\uc81c \uc870\uac74\"\n", "\n", "    \uc774 \uac00\uc774\ub4dc\ub294 \ub2e4\uc74c\uc5d0 \ub300\ud55c \ucda9\ubd84\ud55c \uc774\ud574\ub97c \uac00\uc815\ud569\ub2c8\ub2e4:\n", "    \n", "    - [\uc2a4\ud2b8\ub9ac\ubc0d](../../concepts/streaming/)\n", "    - [\ucc44\ud305 \ubaa8\ub378](https://python.langchain.com/docs/concepts/chat_models/)\n", "    - [\ub3c4\uad6c](https://python.langchain.com/docs/concepts/tools/)\n", "\n", "\uadf8\ub798\ud504\uac00 LLM\uc774\ub098 \uae30\ud0c0 \uc2a4\ud2b8\ub9ac\ubc0d API\ub97c \uc0ac\uc6a9\ud558\ub294 \ub3c4\uad6c\ub97c \ud638\ucd9c\ud558\ub294 \uacbd\uc6b0, \ub3c4\uad6c\uc758 \uc2e4\ud589 \uc911\uc5d0 \ubd80\ubd84 \uacb0\uacfc\ub97c \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \ub3c4\uad6c \uc2e4\ud589 \uc2dc\uac04\uc774 \uae38 \uacbd\uc6b0 \uc720\uc6a9\ud569\ub2c8\ub2e4.\n", "\n", "1. \ub3c4\uad6c \ub0b4\uc5d0\uc11c **\uc784\uc758\uc758** \ub370\uc774\ud130\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud558\ub824\uba74 [`stream_mode=\"custom\"`](../streaming#custom) \ubc0f `get_stream_writer()`\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n", "\n", "    ```python\n", "    # \ub2e4\uc74c \uc904 \uac15\uc870\n", "    from langgraph.config import get_stream_writer\n", "    \n", "    def tool(tool_arg: str):\n", "        writer = get_stream_writer()\n", "        for chunk in custom_data_stream():\n", "            # \uc784\uc758\uc758 \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d\n", "            # \ub2e4\uc74c \uc904 \uac15\uc870\n", "            writer(chunk)\n", "        ...\n", "    \n", "    for chunk in graph.stream(\n", "        inputs,\n", "        # \ub2e4\uc74c \uc904 \uac15\uc870\n", "        stream_mode=\"custom\"\n", "    ):\n", "        print(chunk)\n", "    ```\n", "\n", "2. LLM\uc744 \ud638\ucd9c\ud558\ub294 \ub3c4\uad6c\uc5d0\uc11c \uc0dd\uc131\ub41c LLM \ud1a0\ud070\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\ub824\uba74 [`stream_mode=\"messages\"`](../streaming#messages)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n", "\n", "    ```python\n", "    from langgraph.graph import StateGraph, MessagesState\n", "    from langchain_openai import ChatOpenAI\n", "    \n", "    model = ChatOpenAI()\n", "    \n", "    def tool(tool_arg: str):\n", "        model.invoke(tool_arg)\n", "        ...\n", "    \n", "    def call_tools(state: MessagesState):\n", "        tool_call = get_tool_call(state)\n", "        tool_result = tool(**tool_call[\"args\"])\n", "        ...\n", "    \n", "    graph = (\n", "        StateGraph(MessagesState)\n", "        .add_node(call_tools)\n", "        ...\n", "        .compile()\n", "    \n", "    for msg, metadata in graph.stream(\n", "        inputs,\n", "        # \ub2e4\uc74c \uc904 \uac15\uc870\n", "        stream_mode=\"messages\"\n", "    ):\n", "        print(msg)\n", "    ```\n", "\n", "!!! \ub178\ud2b8 \"LangChain \uc5c6\uc774 \uc0ac\uc6a9\"\n", "\n", "    **LangChain\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0** \ub3c4\uad6c \ub0b4\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud574\uc57c \ud558\ub294 \uacbd\uc6b0, [`stream_mode=\"custom\"`](../streaming/#custom)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [\uc544\ub798 \uc608\uc81c](#example-without-langchain)\ub97c \ud655\uc778\ud558\uc138\uc694.\n", "\n", "!!! \uacbd\uace0 \"Python < 3.11\uc5d0\uc11c\uc758 \ube44\ub3d9\uae30 \ucc98\ub9ac\"\n", "    \n", "    Python < 3.11\uc5d0\uc11c \ube44\ub3d9\uae30 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud560 \ub54c\ub294 `RunnableConfig`\ub97c \uc218\ub3d9\uc73c\ub85c \ucc44\ud305 \ubaa8\ub378\uc5d0 \uc804\ub2ec\ud574\uc57c \ud569\ub2c8\ub2e4. \ud638\ucd9c\ud560 \ub54c \ub2e4\uc74c\uacfc \uac19\uc774 \uc804\ub2ec\ud569\ub2c8\ub2e4: `model.ainvoke(..., config)`. \uc2a4\ud2b8\ub9bc \uba54\uc11c\ub4dc\ub294 \ucf5c\ubc31\uc73c\ub85c \uc804\ub2ec\ub41c \uc2a4\ud2b8\ub9ac\ubc0d \ud2b8\ub808\uc774\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc911\ucca9\ub41c \ucf54\ub4dc\uc758 \ubaa8\ub4e0 \uc774\ubca4\ud2b8\ub97c \uc218\uc9d1\ud569\ub2c8\ub2e4. 3.11 \uc774\uc0c1\uc5d0\uc11c\ub294 \uc774\uac83\uc774 \uc790\ub3d9\uc73c\ub85c \ucc98\ub9ac\ub418\uc9c0\ub9cc, 3.11 \uc774\uc804\uc758 [asyncio\uc758 \uc791\uc5c5](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task)\uc740 \uc801\uc808\ud55c `contextvar` \uc9c0\uc6d0\uc774 \ubd80\uc871\ud558\uc5ec \uc124\uc815\uc744 \uc218\ub3d9\uc73c\ub85c \uc804\ub2ec\ud574\uc57c\ub9cc \ucf5c\ubc31\uc774 \uc804\ud30c\ub429\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc544\ub798\uc758 `call_model` \ud568\uc218\uc5d0\uc11c \uc774\ub97c \ucc98\ub9ac\ud569\ub2c8\ub2e4.\n", "\n", "## \uc124\uc815\n", "\n", "\uba3c\uc800, \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uc124\uce58\ud558\uace0 API \ud0a4\ub97c \uc124\uc815\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "b364dfe2-010b-4588-8489-fb4d8be1f200", "metadata": {}, "outputs": [], "source": ["%%capture --no-stderr\n", "%pip install -U langgraph langchain-openai\n"]}, {"cell_type": "code", "execution_count": 2, "id": "0cf6b41d-7fcb-40b6-9a72-229cdd00a094", "metadata": {}, "outputs": [{"name": "stdin", "output_type": "stream", "text": ["OPENAI_API_KEY:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"]}], "source": ["import getpass\n", "import os\n", "\n", "\n", "def _set_env(var: str):\n", "    if not os.environ.get(var):\n", "        os.environ[var] = getpass.getpass(f\"{var}: \")\n", "\n", "\n", "_set_env(\"OPENAI_API_KEY\")\n"]}, {"cell_type": "markdown", "id": "767cd76a", "metadata": {}, "source": ["<div class=\"admonition tip\">\n", "    <p class=\"admonition-title\">LangGraph \uac1c\ubc1c\uc744 \uc704\ud55c <a href=\"https://smith.langchain.com\">LangSmith</a> \uc124\uc815</p>\n", "    <p style=\"padding-top: 5px;\">\n", "        LangSmith\uc5d0 \uac00\uc785\ud558\uc5ec LangGraph \ud504\ub85c\uc81d\ud2b8\uc758 \ubb38\uc81c\ub97c \uc2e0\uc18d\ud558\uac8c \ud30c\uc545\ud558\uace0 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uc138\uc694. LangSmith\ub294 \ucd94\uc801 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec LangGraph\ub85c \uad6c\ucd95\ub41c LLM \uc571\uc744 \ub514\ubc84\uae45\ud558\uace0 \ud14c\uc2a4\ud2b8\ud558\uba70 \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4 \u2014 \uc2dc\uc791\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 <a href=\"https://docs.smith.langchain.com\">\uc5ec\uae30</a>\ub97c \uc77d\uc5b4\ubcf4\uc138\uc694.\n", "    </p>\n", "</div>\n"]}, {"cell_type": "markdown", "id": "b4ddc3ff-5620-48de-82f0-03b9137410cf", "metadata": {}, "source": ["## \uc0ac\uc6a9\uc790 \uc9c0\uc815 \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d\n", "\n", "\uc774 \uac00\uc774\ub4dc\ub97c \uc704\ud574 [\ubbf8\ub9ac \uad6c\ucd95\ub41c ReAct \uc5d0\uc774\uc804\ud2b8][langgraph.prebuilt.chat_agent_executor.create_react_agent]\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 3, "id": "f1975577-a485-42bd-b0f1-d3e987faf52b", "metadata": {}, "outputs": [], "source": ["from langchain_core.tools import tool\n", "from langchain_openai import ChatOpenAI\n", "\n", "from langgraph.prebuilt import create_react_agent\n", "from langgraph.config import get_stream_writer\n", "\n", "\n", "@tool\n", "async def get_items(place: str) -> str:\n", "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n", "    # highlight-next-line\n", "    writer = get_stream_writer()\n", "\n", "    # this can be replaced with any actual streaming logic that you might have\n", "    items = [\"books\", \"penciles\", \"pictures\"]\n", "    for chunk in items:\n", "        # highlight-next-line\n", "        writer({\"custom_tool_data\": chunk})\n", "\n", "    return \", \".join(items)\n", "\n", "\n", "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n", "tools = [get_items]\n", "# contains `agent` (tool-calling LLM) and `tools` (tool executor) nodes\n", "agent = create_react_agent(llm, tools=tools)\n"]}, {"cell_type": "markdown", "id": "fa96d572-d15f-4f00-b629-cf25e0b4dece", "metadata": {}, "source": ["\uc774\uc81c \ub3c4\uad6c \ud638\ucd9c\uc774 \ud544\uc694\ud55c \uc785\ub825\uc73c\ub85c \uc5d0\uc774\uc804\ud2b8\ub97c \ud638\ucd9c\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 4, "id": "8ae5051c-53b9-4c53-87b2-d7263cda3b7b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'custom_tool_data': 'books'}\n", "{'custom_tool_data': 'penciles'}\n", "{'custom_tool_data': 'pictures'}\n"]}], "source": ["inputs = {\n", "    \"messages\": [  # noqa\n", "        {\"role\": \"user\", \"content\": \"what items are in the office?\"}\n", "    ]\n", "}\n", "async for chunk in agent.astream(\n", "    inputs,\n", "    # highlight-next-line\n", "    stream_mode=\"custom\",\n", "):\n", "    print(chunk)\n"]}, {"cell_type": "markdown", "id": "6d8fa9fc-19af-47d6-9031-ee1720c51aa2", "metadata": {}, "source": ["## \uc2a4\ud2b8\ub9ac\ubc0d LLM \ud1a0\ud070\n"]}, {"cell_type": "code", "execution_count": 5, "id": "38eaf453-9773-424d-a110-9e1038a69805", "metadata": {}, "outputs": [], "source": ["from langchain_core.messages import AIMessageChunk\n", "from langchain_core.runnables import RunnableConfig\n", "\n", "\n", "@tool\n", "async def get_items(\n", "    place: str,\n", "    # Manually accept config (needed for Python <= 3.10)\n", "    # highlight-next-line\n", "    config: RunnableConfig,\n", ") -> str:\n", "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n", "    # Attention: when using async, you should be invoking the LLM using ainvoke!\n", "    # If you fail to do so, streaming will NOT work.\n", "    response = await llm.ainvoke(\n", "        [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": (\n", "                    f\"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n", "                    \"List at least 3 such items separating them by a comma. And include a brief description of each item.\"\n", "                ),\n", "            }\n", "        ],\n", "        # highlight-next-line\n", "        config,\n", "    )\n", "    return response.content\n", "\n", "\n", "tools = [get_items]\n", "# contains `agent` (tool-calling LLM) and `tools` (tool executor) nodes\n", "agent = create_react_agent(llm, tools=tools)\n"]}, {"cell_type": "code", "execution_count": 6, "id": "4c9cdad3-3e9a-444f-9d9d-eae20b8d3486", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Certainly|!| Here| are| three| items| you| might| find| in| a| bedroom|:\n", "\n", "|1|.| **|Bed|**|:| The| central| piece| of| furniture| in| a| bedroom|,| typically| consisting| of| a| mattress| supported| by| a| frame|.| It| is| designed| for| sleeping| and| can| vary| in| size| from| twin| to| king|.| Beds| often| have| bedding|,| including| sheets|,| pillows|,| and| comfort|ers|,| to| enhance| comfort|.\n", "\n", "|2|.| **|D|resser|**|:| A| piece| of| furniture| with| drawers| used| for| storing| clothing| and| personal| items|.| Dress|ers| often| have| a| flat| surface| on| top|,| which| can| be| used| for| decorative| items|,| a| mirror|,| or| personal| accessories|.| They| help| keep| the| bedroom| organized| and| clutter|-free|.\n", "\n", "|3|.| **|Night|stand|**|:| A| small| table| or| cabinet| placed| beside| the| bed|,| used| for| holding| items| such| as| a| lamp|,| alarm| clock|,| books|,| or| personal| items|.| Night|stands| provide| convenience| for| easy| access| to| essentials| during| the| night|,| adding| functionality| and| style| to| the| bedroom| decor|.|"]}], "source": ["inputs = {\n", "    \"messages\": [  # noqa\n", "        {\"role\": \"user\", \"content\": \"what items are in the bedroom?\"}\n", "    ]\n", "}\n", "async for msg, metadata in agent.astream(\n", "    inputs,\n", "    # highlight-next-line\n", "    stream_mode=\"messages\",\n", "):\n", "    if (\n", "        isinstance(msg, AIMessageChunk)\n", "        and msg.content\n", "        # Stream all messages from the tool node\n", "        # highlight-next-line\n", "        and metadata[\"langgraph_node\"] == \"tools\"\n", "    ):\n", "        print(msg.content, end=\"|\", flush=True)\n"]}, {"cell_type": "markdown", "id": "d598d7e2-617d-4c06-bc9a-6a03d5f58499", "metadata": {}, "source": ["## LangChain \uc5c6\ub294 \uc608\uc2dc\n"]}, {"cell_type": "markdown", "id": "780ddcb6-63a7-4c83-a739-bafbe3cd135a", "metadata": {}, "source": ["\ub3c4\uad6c \ud638\ucd9c \ub0b4\uc5d0\uc11c **LangChain\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0** \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc544\ub798 \uc608\uc81c\ub294 \ub2e8\uc77c \ub3c4\uad6c \uc2e4\ud589 \ub178\ub4dc\ub97c \uac00\uc9c4 \uadf8\ub798\ud504\uc5d0\uc11c \uc774\ub97c \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LangChain\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 [ReAct \uc5d0\uc774\uc804\ud2b8\ub97c \ucc98\uc74c\ubd80\ud130 \uad6c\ud604\ud558\ub294 \uac83](../react-agent-from-scratch)\uc740 \ub3c5\uc790\uc5d0\uac8c \uc5f0\uc2b5\uc73c\ub85c \ub0a8\uaca8\ub450\uaca0\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 7, "id": "3e8be67f-4bb8-4f14-9fdb-fc60340f3930", "metadata": {}, "outputs": [], "source": ["import operator\n", "import json\n", "\n", "from typing import TypedDict\n", "from typing_extensions import Annotated\n", "from langgraph.graph import StateGraph, START\n", "\n", "from openai import AsyncOpenAI\n", "\n", "openai_client = AsyncOpenAI()\n", "model_name = \"gpt-4o-mini\"\n", "\n", "\n", "async def stream_tokens(model_name: str, messages: list[dict]):\n", "    response = await openai_client.chat.completions.create(\n", "        messages=messages, model=model_name, stream=True\n", "    )\n", "    role = None\n", "    async for chunk in response:\n", "        delta = chunk.choices[0].delta\n", "\n", "        if delta.role is not None:\n", "            role = delta.role\n", "\n", "        if delta.content:\n", "            yield {\"role\": role, \"content\": delta.content}\n", "\n", "\n", "# this is our tool\n", "async def get_items(place: str) -> str:\n", "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n", "    # highlight-next-line\n", "    writer = get_stream_writer()\n", "    response = \"\"\n", "    async for msg_chunk in stream_tokens(\n", "        model_name,\n", "        [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": (\n", "                    \"Can you tell me what kind of items \"\n", "                    f\"i might find in the following place: '{place}'. \"\n", "                    \"List at least 3 such items separating them by a comma. \"\n", "                    \"And include a brief description of each item.\"\n", "                ),\n", "            }\n", "        ],\n", "    ):\n", "        response += msg_chunk[\"content\"]\n", "        # highlight-next-line\n", "        writer(msg_chunk)\n", "\n", "    return response\n", "\n", "\n", "class State(TypedDict):\n", "    messages: Annotated[list[dict], operator.add]\n", "\n", "\n", "# this is the tool-calling graph node\n", "async def call_tool(state: State):\n", "    ai_message = state[\"messages\"][-1]\n", "    tool_call = ai_message[\"tool_calls\"][-1]\n", "\n", "    function_name = tool_call[\"function\"][\"name\"]\n", "    if function_name != \"get_items\":\n", "        raise ValueError(f\"Tool {function_name} not supported\")\n", "\n", "    function_arguments = tool_call[\"function\"][\"arguments\"]\n", "    arguments = json.loads(function_arguments)\n", "\n", "    function_response = await get_items(**arguments)\n", "    tool_message = {\n", "        \"tool_call_id\": tool_call[\"id\"],\n", "        \"role\": \"tool\",\n", "        \"name\": function_name,\n", "        \"content\": function_response,\n", "    }\n", "    return {\"messages\": [tool_message]}\n", "\n", "\n", "graph = (\n", "    StateGraph(State)  # noqa\n", "    .add_node(call_tool)\n", "    .add_edge(START, \"call_tool\")\n", "    .compile()\n", ")\n"]}, {"cell_type": "markdown", "id": "4e712d12-841c-4eac-a4d8-d01c73c86c8c", "metadata": {}, "source": ["\uc774\uc81c \ub3c4\uad6c \ud638\ucd9c\uc774 \ud3ec\ud568\ub41c AI \uba54\uc2dc\uc9c0\ub85c \uadf8\ub798\ud504\ub97c \ud638\ucd9c\ud574 \ubd05\uc2dc\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 8, "id": "2c30c7b4-62df-4855-8219-d5e1a1a09be9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Sure|!| Here| are| three| common| items| you| might| find| in| a| bedroom|:\n", "\n", "|1|.| **|Bed|**|:| The| focal| point| of| the| bedroom|,| a| bed| typically| consists| of| a| mattress| resting| on| a| frame|,| and| it| may| include| pillows| and| bedding|.| It| provides| a| comfortable| place| for| sleeping| and| resting|.\n", "\n", "|2|.| **|D|resser|**|:| A| piece| of| furniture| with| multiple| drawers|,| a| dresser| is| used| for| storing| clothes|,| accessories|,| and| personal| items|.| It| often| has| a| flat| surface| that| may| be| used| to| display| decorative| items| or| a| mirror|.\n", "\n", "|3|.| **|Night|stand|**|:| Also| known| as| a| bedside| table|,| a| night|stand| is| placed| next| to| the| bed| and| typically| holds| items| like| lamps|,| books|,| alarm| clocks|,| and| personal| belongings| for| convenience| during| the| night|.\n", "\n", "|These| items| contribute| to| the| functionality| and| comfort| of| the| bedroom| environment|.|"]}], "source": ["inputs = {\n", "    \"messages\": [\n", "        {\n", "            \"content\": None,\n", "            \"role\": \"assistant\",\n", "            \"tool_calls\": [\n", "                {\n", "                    \"id\": \"1\",\n", "                    \"function\": {\n", "                        \"arguments\": '{\"place\":\"bedroom\"}',\n", "                        \"name\": \"get_items\",\n", "                    },\n", "                    \"type\": \"function\",\n", "                }\n", "            ],\n", "        }\n", "    ]\n", "}\n", "\n", "async for chunk in graph.astream(\n", "    inputs,\n", "    # highlight-next-line\n", "    stream_mode=\"custom\",\n", "):\n", "    print(chunk[\"content\"], end=\"|\", flush=True)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}}