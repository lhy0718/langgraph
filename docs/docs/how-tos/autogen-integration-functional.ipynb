{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["_\ud55c\uad6d\uc5b4\ub85c \uae30\uacc4\ubc88\uc5ed\ub428_\n\n"]}, {"cell_type": "markdown", "id": "100c0c81-6a9f-4ba1-b1a8-42aae82b7172", "metadata": {}, "source": ["# LangGraph (\ud568\uc218\ud615 API)\uc640 AutoGen, CrewAI \ubc0f \uae30\ud0c0 \ud504\ub808\uc784\uc6cc\ud06c \ud1b5\ud569 \ubc29\ubc95\n", "\n", "LangGraph\ub294 \uc5d0\uc774\uc804\ud2b8 \uae30\ubc18 \ubc0f \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uad6c\ucd95\ud558\uae30 \uc704\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. LangGraph\ub294 \ub2e4\ub978 \uc5d0\uc774\uc804\ud2b8 \ud504\ub808\uc784\uc6cc\ud06c\uc640 \uc27d\uac8c \ud1b5\ud569\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n", "\n", "LangGraph\ub97c \ub2e4\ub978 \uc5d0\uc774\uc804\ud2b8 \ud504\ub808\uc784\uc6cc\ud06c\uc640 \ud1b5\ud569\ud558\uace0\uc790 \ud558\ub294 \uc8fc\uc694 \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n", "\n", "- \ub2e4\uc591\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub85c \uad6c\ucd95\ub41c \uac1c\ubcc4 \uc5d0\uc774\uc804\ud2b8\ub97c \ud3ec\ud568\ud558\ub294 [\ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c](../../concepts/multi_agent)\uc744 \uc0dd\uc131\n", "- LangGraph\ub97c \ud65c\uc6a9\ud558\uc5ec [\uc9c0\uc18d\uc131](../../concepts/persistence), [\uc2a4\ud2b8\ub9ac\ubc0d](../../concepts/streaming), [\ub2e8\uae30 \ubc0f \uc7a5\uae30 \uae30\uc5b5](../../concepts/memory)\uacfc \uac19\uc740 \uae30\ub2a5 \ucd94\uac00\n", "\n", "\ub2e4\ub978 \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc5d0\uc774\uc804\ud2b8\ub97c \ud1b5\ud569\ud558\ub294 \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc740 LangGraph\uc758 [\ub178\ub4dc](../../concepts/low_level/#nodes) \ub0b4\uc5d0\uc11c \ud574\ub2f9 \uc5d0\uc774\uc804\ud2b8\ub97c \ud638\ucd9c\ud558\ub294 \uac83\uc785\ub2c8\ub2e4:\n", "\n", "```python\n", "import autogen\n", "from langgraph.func import entrypoint, task\n", "\n", "autogen_agent = autogen.AssistantAgent(name=\"assistant\", ...)\n", "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", ...)\n", "\n", "@task\n", "def call_autogen_agent(messages):\n", "    response = user_proxy.initiate_chat(\n", "        autogen_agent,\n", "        message=messages[-1],\n", "        ...\n", "    )\n", "    ...\n", "\n", "\n", "@entrypoint()\n", "def workflow(messages):\n", "    response = call_autogen_agent(messages).result()\n", "    return response\n", "\n", "\n", "workflow.invoke(\n", "    [\n", "        {\n", "            \"role\": \"user\",\n", "            \"content\": \"\ud53c\ubcf4\ub098\uce58 \uc218\uc5f4\uc5d0\uc11c 10\uacfc 30 \uc0ac\uc774\uc758 \uc22b\uc790\ub97c \ucc3e\uc544\ub77c\",\n", "        }\n", "    ]\n", ")\n", "```\n", "\n", "\uc774 \uac00\uc774\ub4dc\uc5d0\uc11c\ub294 AutoGen\uacfc \ud1b5\ud569\ub41c LangGraph \ucc57\ubd07\uc744 \uad6c\ucd95\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc, \ub2e4\ub978 \ud504\ub808\uc784\uc6cc\ud06c\uc640\ub3c4 \ub3d9\uc77c\ud55c \uc811\uadfc \ubc29\uc2dd\uc744 \ub530\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "markdown", "id": "b189ceb2-132b-4c7b-81b4-c7b8b062f833", "metadata": {}, "source": ["## \uc124\uc815\n"]}, {"cell_type": "code", "execution_count": 1, "id": "62417d3a-94f9-4a52-9962-12639d714966", "metadata": {}, "outputs": [], "source": ["%pip install autogen langgraph\n"]}, {"cell_type": "code", "execution_count": 2, "id": "d46da41d-0a71-4654-aec8-9e6ad8765236", "metadata": {}, "outputs": [{"name": "stdin", "output_type": "stream", "text": ["OPENAI_API_KEY:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"]}], "source": ["import getpass\n", "import os\n", "\n", "\n", "def _set_env(var: str):\n", "    if not os.environ.get(var):\n", "        os.environ[var] = getpass.getpass(f\"{var}: \")\n", "\n", "\n", "_set_env(\"OPENAI_API_KEY\")\n"]}, {"cell_type": "markdown", "id": "1926bbc3-6b06-41e0-9604-860a2bbf8fa3", "metadata": {}, "source": ["## AutoGen \uc5d0\uc774\uc804\ud2b8 \uc815\uc758\n", "\n", "\uc5ec\uae30\uc5d0\uc11c \uc6b0\ub9ac\uc758 AutoGen \uc5d0\uc774\uc804\ud2b8\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uacf5\uc2dd \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc218\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4 [\uc5ec\uae30](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb).\n"]}, {"cell_type": "code", "execution_count": 3, "id": "524de117-ff09-4b26-bfe8-a9f85a46ffd5", "metadata": {}, "outputs": [], "source": ["import autogen\n", "import os\n", "\n", "config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n", "\n", "llm_config = {\n", "    \"timeout\": 600,\n", "    \"cache_seed\": 42,\n", "    \"config_list\": config_list,\n", "    \"temperature\": 0,\n", "}\n", "\n", "autogen_agent = autogen.AssistantAgent(\n", "    name=\"assistant\",\n", "    llm_config=llm_config,\n", ")\n", "\n", "user_proxy = autogen.UserProxyAgent(\n", "    name=\"user_proxy\",\n", "    human_input_mode=\"NEVER\",\n", "    max_consecutive_auto_reply=10,\n", "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n", "    code_execution_config={\n", "        \"work_dir\": \"web\",\n", "        \"use_docker\": False,\n", "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n", "    llm_config=llm_config,\n", "    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n", ")\n"]}, {"cell_type": "markdown", "id": "8aa858e2-4acb-4f75-be20-b9ccbbcb5073", "metadata": {}, "source": ["---\n"]}, {"cell_type": "markdown", "id": "dcc478f5-4a35-43f8-bf59-9cb71289cd00", "metadata": {}, "source": ["## \uc6cc\ud06c\ud50c\ub85c\uc6b0 \uc0dd\uc131\n", "\n", "\uc774\uc81c AutoGen \uc5d0\uc774\uc804\ud2b8\ub97c \ud638\ucd9c\ud558\ub294 LangGraph \ucc57\ubd07 \uadf8\ub798\ud504\ub97c \uc0dd\uc131\ud560 \uac83\uc785\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 8, "id": "d129e4e1-3766-429a-b806-cde3d8bc0469", "metadata": {}, "outputs": [], "source": ["from langchain_core.messages import convert_to_openai_messages, BaseMessage\n", "from langgraph.func import entrypoint, task\n", "from langgraph.graph import add_messages\n", "from langgraph.checkpoint.memory import MemorySaver\n", "\n", "\n", "@task\n", "def call_autogen_agent(messages: list[BaseMessage]):\n", "    # convert to openai-style messages\n", "    messages = convert_to_openai_messages(messages)\n", "    response = user_proxy.initiate_chat(\n", "        autogen_agent,\n", "        message=messages[-1],\n", "        # pass previous message history as context\n", "        carryover=messages[:-1],\n", "    )\n", "    # get the final response from the agent\n", "    content = response.chat_history[-1][\"content\"]\n", "    return {\"role\": \"assistant\", \"content\": content}\n", "\n", "\n", "# add short-term memory for storing conversation history\n", "checkpointer = MemorySaver()\n", "\n", "\n", "@entrypoint(checkpointer=checkpointer)\n", "def workflow(messages: list[BaseMessage], previous: list[BaseMessage]):\n", "    messages = add_messages(previous or [], messages)\n", "    response = call_autogen_agent(messages).result()\n", "    return entrypoint.final(value=response, save=add_messages(messages, response))\n"]}, {"cell_type": "markdown", "id": "23d629c3-1d6b-40af-adf6-915e15657566", "metadata": {}, "source": ["## \uadf8\ub798\ud504 \uc2e4\ud589\ud558\uae30\n", "\n", "\uc774\uc81c \uadf8\ub798\ud504\ub97c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 10, "id": "a279b667-0f5d-4008-8d43-c806a3f379c4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[33muser_proxy\u001b[0m (to assistant):\n", "\n", "Find numbers between 10 and 30 in fibonacci sequence\n", "\n", "--------------------------------------------------------------------------------\n", "\u001b[33massistant\u001b[0m (to user_proxy):\n", "\n", "To find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:\n", "\n", "1. Generate Fibonacci numbers starting from 0.\n", "2. Continue generating until the numbers exceed 30.\n", "3. Collect and print the numbers that are between 10 and 30.\n", "\n", "Let's implement this in Python:\n", "\n", "```python\n", "# filename: fibonacci_range.py\n", "\n", "def fibonacci_sequence():\n", "    a, b = 0, 1\n", "    while a <= 30:\n", "        if 10 <= a <= 30:\n", "            print(a)\n", "        a, b = b, a + b\n", "\n", "fibonacci_sequence()\n", "```\n", "\n", "This script will print the Fibonacci numbers between 10 and 30. Please execute the code to see the result.\n", "\n", "--------------------------------------------------------------------------------\n", "\u001b[31m\n", ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n", "\u001b[33muser_proxy\u001b[0m (to assistant):\n", "\n", "exitcode: 0 (execution succeeded)\n", "Code output: \n", "13\n", "21\n", "\n", "\n", "--------------------------------------------------------------------------------\n", "\u001b[33massistant\u001b[0m (to user_proxy):\n", "\n", "The Fibonacci numbers between 10 and 30 are 13 and 21. \n", "\n", "These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n", "\n", "The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n", "\n", "As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n", "\n", "TERMINATE\n", "\n", "--------------------------------------------------------------------------------\n", "{'call_autogen_agent': {'role': 'assistant', 'content': 'The Fibonacci numbers between 10 and 30 are 13 and 21. \\n\\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \\n\\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\n\\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\\n\\nTERMINATE'}}\n", "{'workflow': {'role': 'assistant', 'content': 'The Fibonacci numbers between 10 and 30 are 13 and 21. \\n\\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \\n\\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\n\\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\\n\\nTERMINATE'}}\n"]}], "source": ["# pass the thread ID to persist agent outputs for future interactions\n", "# highlight-next-line\n", "config = {\"configurable\": {\"thread_id\": \"1\"}}\n", "\n", "for chunk in workflow.stream(\n", "    [\n", "        {\n", "            \"role\": \"user\",\n", "            \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n", "        }\n", "    ],\n", "    # highlight-next-line\n", "    config,\n", "):\n", "    print(chunk)\n"]}, {"cell_type": "markdown", "id": "c6cd57b4-d4ee-49f6-be12-318613849669", "metadata": {}, "source": ["LangGraph\uc758 [\uc9c0\uc18d\uc131](https://langchain-ai.github.io/langgraph/concepts/persistence/) \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uace0 \uc788\uc73c\ubbc0\ub85c, \uc774\uc81c \ub3d9\uc77c\ud55c \uc2a4\ub808\ub4dc ID\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub300\ud654\ub97c \uacc4\uc18d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. LangGraph\ub294 \uc774\uc804 \uae30\ub85d\uc744 \uc790\ub3d9\uc73c\ub85c AutoGen \uc5d0\uc774\uc804\ud2b8\uc5d0 \uc804\ub2ec\ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 12, "id": "e68811a7-962e-4fe3-9f45-9b99ebbe04e7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[33muser_proxy\u001b[0m (to assistant):\n", "\n", "Multiply the last number by 3\n", "Context: \n", "Find numbers between 10 and 30 in fibonacci sequence\n", "The Fibonacci numbers between 10 and 30 are 13 and 21. \n", "\n", "These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n", "\n", "The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n", "\n", "As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n", "\n", "TERMINATE\n", "\n", "--------------------------------------------------------------------------------\n", "\u001b[33massistant\u001b[0m (to user_proxy):\n", "\n", "The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n", "\n", "21 * 3 = 63\n", "\n", "TERMINATE\n", "\n", "--------------------------------------------------------------------------------\n", "{'call_autogen_agent': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}\n", "{'workflow': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}\n"]}], "source": ["for chunk in workflow.stream(\n", "    [\n", "        {\n", "            \"role\": \"user\",\n", "            \"content\": \"Multiply the last number by 3\",\n", "        }\n", "    ],\n", "    # highlight-next-line\n", "    config,\n", "):\n", "    print(chunk)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 5}