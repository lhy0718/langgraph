{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \uc0ac\uc6a9\uc790 \uc785\ub825\uc744 \uae30\ub2e4\ub9ac\ub294 \ubc29\ubc95 (Functional API)\n", "\n", "!!! \uc815\ubcf4 \"\uc804\uc81c \uc870\uac74\"\n", "    \uc774 \uac00\uc774\ub4dc\ub294 \ub2e4\uc74c\uc5d0 \ub300\ud55c \uc9c0\uc2dd\uc774 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4:\n", "\n", "    - [\ud734\uba3c \uc778 \ub354 \ub8e8\ud504](../../concepts/human_in_the_loop) \uc6cc\ud06c\ud50c\ub85c\uc6b0 \uad6c\ud604 [\uc778\ud130\ub7fd\ud2b8](../../concepts/human_in_the_loop/#interrupt)\n", "    - [Functional API\ub97c \uc0ac\uc6a9\ud558\uc5ec ReAct \uc5d0\uc774\uc804\ud2b8 \uc0dd\uc131\ud558\ub294 \ubc29\ubc95](../../how-tos/react-agent-from-scratch-functional)\n", "\n", "**\ud734\uba3c \uc778 \ub354 \ub8e8\ud504 (HIL)** \uc0c1\ud638\uc791\uc6a9\uc740 [\uc5d0\uc774\uc804\ud2f1 \uc2dc\uc2a4\ud15c](../../concepts/agentic_concepts/#human-in-the-loop)\uc5d0\uc11c \uc911\uc694\ud569\ub2c8\ub2e4. \uc778\uac04 \uc785\ub825\uc744 \uae30\ub2e4\ub9ac\ub294 \uac83\uc740 \uc77c\ubc18\uc801\uc778 HIL \uc0c1\ud638\uc791\uc6a9 \ud328\ud134\uc73c\ub85c, \uc5d0\uc774\uc804\ud2b8\uac00 \uc0ac\uc6a9\uc790\uc5d0\uac8c \uba85\ud655\ud55c \uc9c8\ubb38\uc744 \ud558\uace0 \uc785\ub825\uc744 \uae30\ub2e4\ub9b0 \ub2e4\uc74c \uc9c4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n", "\n", "\uc6b0\ub9ac\ub294 LangGraph\uc5d0\uc11c [interrupt]() \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ub97c \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. `interrupt`\ub294 \uadf8\ub798\ud504 \uc2e4\ud589\uc744 \uba48\ucd94\uace0 \uc0ac\uc6a9\uc790\ub85c\ubd80\ud130 \uc785\ub825\uc744 \uc218\uc9d1\ud55c \ud6c4 \uc218\uc9d1\ub41c \uc785\ub825\uc73c\ub85c \uc2e4\ud589\uc744 \uacc4\uc18d\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4.\n", "\n", "\uc774 \uac00\uc774\ub4dc\ub294 LangGraph\uc758 [Functional API](../../concepts/functional_api)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud734\uba3c \uc778 \ub354 \ub8e8\ud504 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \uad6c\ud604\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, \ub2e4\uc74c\uc744 \uc2dc\uc5f0\ud560 \uac83\uc785\ub2c8\ub2e4:\n", "\n", "1. [\uac04\ub2e8\ud55c \uc0ac\uc6a9 \uc608\uc2dc](#simple-usage)\n", "2. [ReAct \uc5d0\uc774\uc804\ud2b8\uc640\uc758 \uc0ac\uc6a9 \ubc29\ubc95](#agent)\n", "\n", "## \uc124\uc815\n", "\n", "\uba3c\uc800, \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uc124\uce58\ud558\uace0 API \ud0a4\ub97c \uc124\uc815\ud569\uc2dc\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["%%capture --no-stderr\n", "%pip install -U langgraph langchain-openai\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import getpass\n", "import os\n", "\n", "\n", "def _set_env(var: str):\n", "    if not os.environ.get(var):\n", "        os.environ[var] = getpass.getpass(f\"{var}: \")\n", "\n", "\n", "_set_env(\"OPENAI_API_KEY\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"admonition tip\">\n", "     <p class=\"admonition-title\">\ub354 \ub098\uc740 \ub514\ubc84\uae45\uc744 \uc704\ud574 <a href=\"https://smith.langchain.com\">LangSmith</a>\ub97c \uc124\uc815\ud558\uc138\uc694</p>\n", "     <p style=\"padding-top: 5px;\">\n", "         LangSmith\uc5d0 \uac00\uc785\ud558\uc5ec \ubb38\uc81c\ub97c \uc2e0\uc18d\ud558\uac8c \ud30c\uc545\ud558\uace0 LangGraph \ud504\ub85c\uc81d\ud2b8\uc758 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uc138\uc694. LangSmith\ub294 LangGraph\ub85c \uad6c\ucd95\ub41c LLM \uc571\uc744 \ub514\ubc84\uae45\ud558\uace0, \ud14c\uc2a4\ud2b8\ud558\uba70 \ubaa8\ub2c8\ud130\ub9c1\ud558\uae30 \uc704\ud574 \ucd94\uc801 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4 \u2014 \uc2dc\uc791\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 <a href=\"https://docs.smith.langchain.com\">\ubb38\uc11c</a>\ub97c \ucc38\uace0\ud558\uc138\uc694.\n", "     </p>\n", "</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \uac04\ub2e8\ud55c \uc0ac\uc6a9\ubc95\n", "\n", "\uac04\ub2e8\ud55c \uc0ac\uc6a9 \uc608\uc81c\ub97c \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc138 \uac00\uc9c0 [\uc791\uc5c5](../../concepts/functional_api/#task)\uc744 \uc0dd\uc131\ud560 \uac83\uc785\ub2c8\ub2e4:\n", "\n", "1. `\"bar\"`\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\n", "2. \uc778\uac04 \uc785\ub825\uc744 \uc704\ud55c \uc77c\uc2dc \uc911\uc9c0. \uc7ac\uac1c\ud560 \ub54c \uc778\uac04 \uc785\ub825\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4.\n", "3. `\"qux\"`\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from langgraph.func import entrypoint, task\n", "from langgraph.types import Command, interrupt\n", "\n", "\n", "@task\n", "def step_1(input_query):\n", "    \"\"\"Append bar.\"\"\"\n", "    return f\"{input_query} bar\"\n", "\n", "\n", "@task\n", "def human_feedback(input_query):\n", "    \"\"\"Append user input.\"\"\"\n", "    feedback = interrupt(f\"Please provide feedback: {input_query}\")\n", "    return f\"{input_query} {feedback}\"\n", "\n", "\n", "@task\n", "def step_3(input_query):\n", "    \"\"\"Append qux.\"\"\"\n", "    return f\"{input_query} qux\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc774\uc81c \uc774\ub7ec\ud55c \uc791\uc5c5\uc744 \uac04\ub2e8\ud55c [\uc5d4\ud2b8\ub9ac\ud3ec\uc778\ud2b8](../../concepts/functional_api/#entrypoint)\uc5d0\uc11c \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["from langgraph.checkpoint.memory import MemorySaver\n", "\n", "checkpointer = MemorySaver()\n", "\n", "\n", "@entrypoint(checkpointer=checkpointer)\n", "def graph(input_query):\n", "    result_1 = step_1(input_query).result()\n", "    result_2 = human_feedback(result_1).result()\n", "    result_3 = step_3(result_2).result()\n", "\n", "    return result_3\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc6b0\ub9ac\uac00 \uc778\uac04-\ub8e8\ud504 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \ud65c\uc131\ud654\ud558\uae30 \uc704\ud574 \uc218\ud589\ud55c \ubaa8\ub4e0 \uc791\uc5c5\uc740 \uc791\uc5c5 \ub0b4\uc5d0\uc11c [interrupt()](../../concepts/human_in_the_loop/#interrupt)\uc774\ub77c\uace0 \ubd88\ub9bd\ub2c8\ub2e4.\n", "\n", "!!! \ud301\n", "\n", "    \uc774\uc804 \uc791\uc5c5\uc758 \uacb0\uacfc-- \uc774 \uacbd\uc6b0 `step_1`--\uc774 \uc720\uc9c0\ub418\ubbc0\ub85c `interrupt` \uc774\ud6c4\uc5d0 \ub2e4\uc2dc \uc2e4\ud589\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n", "\n", "\ucffc\ub9ac \ubb38\uc790\uc5f4\uc744 \ubcf4\ub0b4\ubd05\uc2dc\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["config = {\"configurable\": {\"thread_id\": \"1\"}}\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'step_1': 'foo bar'}\n", "\n", "\n", "{'__interrupt__': (Interrupt(value='Please provide feedback: foo bar', resumable=True, ns=['graph:d66b2e35-0ee3-d8d6-1a22-aec9d58f13b9', 'human_feedback:e0cd4ee2-b874-e1d2-8bc4-3f7ddc06bcc2'], when='during'),)}\n", "\n", "\n"]}], "source": ["for event in graph.stream(\"foo\", config):\n", "    print(event)\n", "    print(\"\\n\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`step_1` \uc774\ud6c4\uc5d0 `interrupt`\ub85c \uc77c\uc2dc \uc911\uc9c0\ud588\uc74c\uc744 \uc720\uc758\ud558\uc2ed\uc2dc\uc624. \uc774 \uc778\ud130\ub7fd\ud2b8\ub294 \uc2e4\ud589\uc744 \uc7ac\uac1c\ud558\uae30 \uc704\ud55c \uc9c0\uce68\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc7ac\uac1c\ud558\ub824\uba74 `human_feedback` \uc791\uc5c5\uc5d0\uc11c \uae30\ub300\ub418\ub294 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub41c [Command](../../concepts/human_in_the_loop/#the-command-primitive)\ub97c \ubc1c\ud589\ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'human_feedback': 'foo bar baz'}\n", "\n", "\n", "{'step_3': 'foo bar baz qux'}\n", "\n", "\n", "{'graph': 'foo bar baz qux'}\n", "\n", "\n"]}], "source": ["# Continue execution\n", "for event in graph.stream(Command(resume=\"baz\"), config):\n", "    print(event)\n", "    print(\"\\n\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc7ac\uac1c\ud55c \ud6c4\uc5d0 \uc2e4\ud589\uc740 \ub0a8\uc740 \ub2e8\uacc4\ub85c \uc9c4\ud589\ub418\uba70 \uc608\uc0c1\ub300\ub85c \uc885\ub8cc\ub429\ub2c8\ub2e4.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \uc5d0\uc774\uc804\ud2b8\n", "\n", "[\uae30\ub2a5\uc801 API\ub97c \uc0ac\uc6a9\ud558\uc5ec ReAct \uc5d0\uc774\uc804\ud2b8\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\ubc95](../../how-tos/react-agent-from-scratch-functional) \uac00\uc774\ub4dc\ub97c \ubc14\ud0d5\uc73c\ub85c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n", "\n", "\uc5ec\uae30\uc11c\ub294 \ud544\uc694\ud560 \ub54c \uc778\uac04\uc5d0\uac8c \ub3c4\uc6c0\uc744 \uc694\uccad\ud560 \uc218 \uc788\ub3c4\ub85d \uc5d0\uc774\uc804\ud2b8\ub97c \ud655\uc7a5\ud560 \uac83\uc785\ub2c8\ub2e4.\n", "\n", "### \ubaa8\ub378 \ubc0f \ub3c4\uad6c \uc815\uc758\n", "\n", "\uc608\uc81c\ub97c \uc704\ud574 \uc0ac\uc6a9\ud560 \ub3c4\uad6c\uc640 \ubaa8\ub378\uc744 \uba3c\uc800 \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. [ReAct \uc5d0\uc774\uc804\ud2b8 \uac00\uc774\ub4dc](../../how-tos/react-agent-from-scratch-functional)\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c, \ud2b9\uc815 \uc704\uce58\uc758 \ub0a0\uc528\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc5bb\ub294 \ub2e8\uc77c \uc790\ub9ac \ud45c\uc2dc\uc790 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n", "\n", "\uc774 \uc608\uc81c\uc5d0\uc11c\ub294 [OpenAI](https://python.langchain.com/docs/integrations/providers/openai/) \ucc44\ud305 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\uc9c0\ub9cc, \ub3c4\uad6c \ud638\ucd9c\uc744 \uc9c0\uc6d0\ud558\ub294 \uc5b4\ub5a4 \ubaa8\ub378\ub3c4 [\uc0ac\uc6a9 \uac00\ub2a5](https://python.langchain.com/docs/integrations/chat/)\ud569\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["from langchain_openai import ChatOpenAI\n", "from langchain_core.tools import tool\n", "\n", "model = ChatOpenAI(model=\"gpt-4o-mini\")\n", "\n", "\n", "@tool\n", "def get_weather(location: str):\n", "    \"\"\"Call to get the weather from a specific location.\"\"\"\n", "    # This is a placeholder for the actual implementation\n", "    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n", "        return \"It's sunny!\"\n", "    elif \"boston\" in location.lower():\n", "        return \"It's rainy!\"\n", "    else:\n", "        return f\"I am not sure what the weather is in {location}\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\ub3c4\uc6c0\uc744 \ubc1b\uae30 \uc704\ud574 \uc0ac\ub78c\uc5d0\uac8c \uc5f0\ub77d\ud558\ub824\uba74, \ub2e8\uc21c\ud788 [interrupt](../../concepts/human_in_the_loop/#interrupt)\ub97c \ud638\ucd9c\ud558\ub294 \ub3c4\uad6c\ub97c \ucd94\uac00\ud558\uba74 \ub429\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["from langgraph.types import Command, interrupt\n", "\n", "\n", "@tool\n", "def human_assistance(query: str) -> str:\n", "    \"\"\"Request assistance from a human.\"\"\"\n", "    human_response = interrupt({\"query\": query})\n", "    return human_response[\"data\"]\n", "\n", "\n", "tools = [get_weather, human_assistance]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \uc791\uc5c5 \uc815\uc758\n", "\n", "\uc6b0\ub9ac\uc758 \uc791\uc5c5\uc740 [ReAct \uc5d0\uc774\uc804\ud2b8 \uac00\uc774\ub4dc](../../how-tos/react-agent-from-scratch-functional)\uc640 \ub3d9\uc77c\ud569\ub2c8\ub2e4:\n", "\n", "1. **\ubaa8\ub378 \ud638\ucd9c**: \uc6b0\ub9ac\ub294 \uba54\uc2dc\uc9c0 \ubaa9\ub85d\uc73c\ub85c \uc6b0\ub9ac\uc758 \ucc44\ud305 \ubaa8\ub378\uc5d0 \ucffc\ub9ac\ub97c \ud558\uace0\uc790 \ud569\ub2c8\ub2e4.\n", "2. **\ub3c4\uad6c \ud638\ucd9c**: \ub9cc\uc57d \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc774 \ub3c4\uad6c \ud638\ucd9c\uc744 \uc0dd\uc131\ud558\uba74, \uc6b0\ub9ac\ub294 \uadf8\uac83\uc744 \uc2e4\ud589\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.\n", "\n", "\uc6b0\ub9ac\ub294 \ubaa8\ub378\uc774 \uc811\uadfc\ud560 \uc218 \uc788\ub294 \ub610 \ud558\ub098\uc758 \ub3c4\uad6c\ub9cc \ub354 \uc788\uc2b5\ub2c8\ub2e4.\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["from langchain_core.messages import ToolMessage\n", "from langgraph.func import entrypoint, task\n", "\n", "tools_by_name = {tool.name: tool for tool in tools}\n", "\n", "\n", "@task\n", "def call_model(messages):\n", "    \"\"\"Call model with a sequence of messages.\"\"\"\n", "    response = model.bind_tools(tools).invoke(messages)\n", "    return response\n", "\n", "\n", "@task\n", "def call_tool(tool_call):\n", "    tool = tools_by_name[tool_call[\"name\"]]\n", "    observation = tool.invoke(tool_call)\n", "    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \uc9c4\uc785\uc810 \uc815\uc758\n", "\n", "\uc6b0\ub9ac\uc758 [\uc9c4\uc785\uc810](../../concepts/functional_api/#entrypoint)\uc740 [ReAct \uc5d0\uc774\uc804\ud2b8 \uac00\uc774\ub4dc](../../how-tos/react-agent-from-scratch-functional)\uc640 \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ub429\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["from langgraph.checkpoint.memory import MemorySaver\n", "from langgraph.graph.message import add_messages\n", "\n", "checkpointer = MemorySaver()\n", "\n", "\n", "@entrypoint(checkpointer=checkpointer)\n", "def agent(messages, previous):\n", "    if previous is not None:\n", "        messages = add_messages(previous, messages)\n", "\n", "    llm_response = call_model(messages).result()\n", "    while True:\n", "        if not llm_response.tool_calls:\n", "            break\n", "\n", "        # Execute tools\n", "        tool_result_futures = [\n", "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n", "        ]\n", "        tool_results = [fut.result() for fut in tool_result_futures]\n", "\n", "        # Append to message list\n", "        messages = add_messages(messages, [llm_response, *tool_results])\n", "\n", "        # Call model again\n", "        llm_response = call_model(messages).result()\n", "\n", "    # Generate final response\n", "    messages = add_messages(messages, llm_response)\n", "    return entrypoint.final(value=llm_response, save=messages)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \uc0ac\uc6a9\ubc95\n", "\n", "\ubaa8\ub378\uc744 \uc778\uac04\uc758 \ub3c4\uc6c0\uc774 \ud544\uc694\ud55c \uc9c8\ubb38\uc73c\ub85c \ud638\ucd9c\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \uc9c8\ubb38\uc740 \ub610\ud55c `get_weather` \ub3c4\uad6c\uc758 \ud638\ucd9c\uc744 \uc694\uad6c\ud560 \uac83\uc785\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["def _print_step(step: dict) -> None:\n", "    for task_name, result in step.items():\n", "        if task_name == \"agent\":\n", "            continue  # just stream from tasks\n", "        print(f\"\\n{task_name}:\")\n", "        if task_name == \"__interrupt__\":\n", "            print(result)\n", "        else:\n", "            result.pretty_print()\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["config = {\"configurable\": {\"thread_id\": \"1\"}}\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'role': 'user', 'content': 'Can you reach out for human assistance: what should I feed my cat? Separately, can you check the weather in San Francisco?'}\n", "\n", "call_model:\n", "==================================\u001b[1m Ai Message \u001b[0m==================================\n", "Tool Calls:\n", "  human_assistance (call_joAEBVX7Abfm7TsZ0k95ZkVx)\n", " Call ID: call_joAEBVX7Abfm7TsZ0k95ZkVx\n", "  Args:\n", "    query: What should I feed my cat?\n", "  get_weather (call_ut7zfHFCcms63BOZLrRHszGH)\n", " Call ID: call_ut7zfHFCcms63BOZLrRHszGH\n", "  Args:\n", "    location: San Francisco\n", "\n", "call_tool:\n", "=================================\u001b[1m Tool Message \u001b[0m=================================\n", "\n", "content=\"It's sunny!\" name='get_weather' tool_call_id='call_ut7zfHFCcms63BOZLrRHszGH'\n", "\n", "__interrupt__:\n", "(Interrupt(value={'query': 'What should I feed my cat?'}, resumable=True, ns=['agent:aa676ccc-b038-25e3-9c8a-18e81d4e1372', 'call_tool:059d53d2-3344-13bc-e170-48b632c2dd97'], when='during'),)\n"]}], "source": ["user_message = {\n", "    \"role\": \"user\",\n", "    \"content\": (\n", "        \"Can you reach out for human assistance: what should I feed my cat? \"\n", "        \"Separately, can you check the weather in San Francisco?\"\n", "    ),\n", "}\n", "print(user_message)\n", "\n", "for step in agent.stream([user_message], config):\n", "    _print_step(step)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc6b0\ub9ac\uac00 \ub450 \uac1c\uc758 \ub3c4\uad6c \ud638\ucd9c\uc744 \uc0dd\uc131\ud588\uc74c\uc744 \uc8fc\uc758\ud558\uc138\uc694. \uc6b0\ub9ac\uc758 \uc2e4\ud589\uc774 \uc911\ub2e8\ub418\uc5c8\uc9c0\ub9cc, `get_weather` \ub3c4\uad6c\uc758 \uc2e4\ud589\uc740 \ucc28\ub2e8\ud558\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\n", "\n", "\uc6b0\ub9ac\uac00 \uc911\ub2e8\ub41c \uacf3\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'__interrupt__': (Interrupt(value={'query': 'What should I feed my cat?'}, resumable=True, ns=['agent:aa676ccc-b038-25e3-9c8a-18e81d4e1372', 'call_tool:059d53d2-3344-13bc-e170-48b632c2dd97'], when='during'),)}\n"]}], "source": ["print(step)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc6b0\ub9ac\ub294 [\uba85\ub839](../../concepts/human_in_the_loop/#the-command-primitive)\uc744 \ubc1c\ud589\ud568\uc73c\ub85c\uc368 \uc2e4\ud589\uc744 \uc7ac\uac1c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. `Command`\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ub370\uc774\ud130\ub294 `human_assistance`\uc758 \uad6c\ud604\uc5d0 \ub530\ub77c \uadc0\ud558\uc758 \uc694\uad6c\uc5d0 \ub9de\uac8c \uc0ac\uc6a9\uc790 \uc815\uc758\ub420 \uc218 \uc788\uc74c\uc744 \uc720\uc758\ud558\uc138\uc694.\n"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "call_tool:\n", "=================================\u001b[1m Tool Message \u001b[0m=================================\n", "\n", "content='You should feed your cat a fish.' name='human_assistance' tool_call_id='call_joAEBVX7Abfm7TsZ0k95ZkVx'\n", "\n", "call_model:\n", "==================================\u001b[1m Ai Message \u001b[0m==================================\n", "\n", "For human assistance, you should feed your cat fish. \n", "\n", "Regarding the weather in San Francisco, it's sunny!\n"]}], "source": ["human_response = \"You should feed your cat a fish.\"\n", "human_command = Command(resume={\"data\": human_response})\n", "\n", "for step in agent.stream(human_command, config):\n", "    _print_step(step)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\uc704\uc5d0\uc11c \uc7ac\uac1c\ud560 \ub54c \ucd5c\uc885 \ub3c4\uad6c \uba54\uc2dc\uc9c0\ub97c \uc81c\uacf5\ud558\uc5ec \ubaa8\ub378\uc774 \uc751\ub2f5\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc804\uccb4 \uc2e4\ud589 \ud750\ub984\uc744 \ubcf4\ub824\uba74 LangSmith \ucd94\uc801\uc744 \ud655\uc778\ud558\uc138\uc694:\n", "\n", "1. [\ucd08\uae30 \ucffc\ub9ac\uc5d0\uc11c\uc758 \ucd94\uc801](https://smith.langchain.com/public/c3d8879d-4d01-41be-807e-6d9eed15df99/r)\n", "2. [\uc7ac\uac1c \ud6c4\uc758 \ucd94\uc801](https://smith.langchain.com/public/97c05ef9-8b4c-428e-8826-3fd417c8c75f/r)\n"]}], "metadata": {"translated_ko": true}}